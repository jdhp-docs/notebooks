{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "meta",
     "draft"
    ]
   },
   "source": [
    "# Perceptron Multicouche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auteur: Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jdhp-docs/notebooks/master?urlpath=apps%2Fnb_sci_ai%2Fai_ml_multilayer_perceptron_fr.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import nnfigs\n",
    "\n",
    "# https://github.com/jeremiedecock/neural-network-figures.git\n",
    "import nnfigs.core as nnfig\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# TODO\n",
    "\n",
    "# Shallow et Deep learning à lire:\n",
    "# - https://www.miximum.fr/blog/introduction-au-deep-learning-2/\n",
    "# - https://sciencetonnante.wordpress.com/2016/04/08/le-deep-learning/\n",
    "# - https://www.technologies-ebusiness.com/enjeux-et-tendances/le-deep-learning-pas-a-pas\n",
    "# - http://scholar.google.fr/scholar_url?url=https://arxiv.org/pdf/1404.7828&hl=fr&sa=X&scisig=AAGBfm07Y2UDlPpbninerh4gxHUj2SJfDQ&nossl=1&oi=scholarr&sqi=2&ved=0ahUKEwjfxMu7jKnUAhUoCsAKHR_RDlkQgAMIKygAMAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\cur}{i}\n",
    "\\newcommand{\\prev}{j}\n",
    "\\newcommand{\\prevcur}{{\\cur\\prev}}\n",
    "\\newcommand{\\next}{k}\n",
    "\\newcommand{\\curnext}{{\\next\\cur}}\n",
    "\\newcommand{\\ex}{\\eta}\n",
    "\\newcommand{\\pot}{\\rho}\n",
    "\\newcommand{\\feature}{x}\n",
    "\\newcommand{\\weight}{w}\n",
    "\\newcommand{\\wcur}{{\\weight_{\\cur\\prev}}}\n",
    "\\newcommand{\\activthres}{\\theta}\n",
    "\\newcommand{\\activfunc}{f}\n",
    "\\newcommand{\\errfunc}{E}\n",
    "\\newcommand{\\learnrate}{\\epsilon}\n",
    "\\newcommand{\\learnit}{n}\n",
    "\\newcommand{\\sigout}{y}\n",
    "\\newcommand{\\sigoutdes}{d}\n",
    "\\newcommand{\\weights}{\\boldsymbol{W}}\n",
    "\\newcommand{\\errsig}{\\Delta}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notations :\n",
    "# - $\\cur$: couche courante\n",
    "# - $\\prev$: couche immédiatement en amont de la courche courrante (i.e. vers la couche d'entrée du réseau)\n",
    "# - $\\next$: couche immédiatement en aval de la courche courrante (i.e. vers la couche de sortie du réseau)\n",
    "# - $\\ex$: exemple (*sample* ou *feature*) courant (i.e. le vecteur des entrées courantes du réseau)\n",
    "# - $\\pot_\\cur$: *Potentiel d'activation* du neurone $i$ pour l'exemple courant\n",
    "# - $\\wcur$: Poids de la connexion entre le neurone $j$ et le neurone $i$\n",
    "# - $\\activthres_\\cur$: *Seuil d'activation* du neurone $i$\n",
    "# - $\\activfunc_\\cur$: *Fonction d'activation* du neurone $i$\n",
    "# - $\\errfunc$: *Fonction objectif* ou *fonction d'erreur*\n",
    "# - $\\learnrate$: *Pas d'apprentissage* ou *Taux d'apprentissage*\n",
    "# - $\\learnit$: Numéro d'itération (ou cycle ou époque) du processus d'apprentissage\n",
    "# - $\\sigout_\\cur$: Signal de sortie du neurone $i$ pour l'exemple courant\n",
    "# - $\\sigoutdes_\\cur$: Sortie désirée (*étiquette*) du neurone $i$ pour l'exemple courant\n",
    "# - $\\weights$: Matrice des poids du réseau (en réalité il y a une matrice de taille potentiellement différente par couche)\n",
    "# - $\\errsig_i$: *Signal d'erreur* du neurone $i$ pour l'exemple courant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qu'est-ce qu'un réseau de neurones ?\n",
    "\n",
    "Une grosse fonction parametrique.\n",
    "Pour peu qu'on donne suffisamment de paramètres à cette fonction, elle est capable d'approximer n'importe quelle fonction continue.\n",
    "\n",
    "Représentation schématique d'une fonction paramètrique avec 3 paramètres avec une entrée en une sortie à 1 dimension\n",
    "\n",
    "$$\\mathbb{R} \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "$$x \\mapsto g_{\\boldsymbol{\\omega}}(x)$$\n",
    "\n",
    "TODO: image/schéma intuition : entrés -> fonction avec paramètres = table de mixage -> sortie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### À quoi ça sert ?\n",
    "\n",
    "TODO : expliquer la régression et la classification\n",
    "\n",
    "TODO : applications avec références...\n",
    "Exemples d'application concrètes :\n",
    "- Reconnaissance de texte manuscrit\n",
    "- Reconnaissance de formes, d'objets, de visages, etc. dans des images\n",
    "- Reconnaissance de la parole\n",
    "- Prédiction de séries temporelles (cours de la bourse, etc.)\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Définition du neurone \"formel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "STR_CUR = r\"i\"       # Couche courante\n",
    "STR_PREV = r\"j\"      # Couche immédiatement en amont de la courche courrante (i.e. vers la couche d'entrée du réseau)\n",
    "STR_NEXT = r\"k\"      # Couche immédiatement en aval de la courche courrante (i.e. vers la couche de sortie du réseau)\n",
    "STR_EX = r\"\\eta\"     # Exemple (*sample* ou *feature*) courant (i.e. le vecteur des entrées courantes du réseau)\n",
    "STR_POT = r\"x\"       # *Potentiel d'activation* du neurone $i$ pour l'exemple $\\ex$\n",
    "STR_POT_CUR = r\"x_i\"       # *Potentiel d'activation* du neurone $i$ pour l'exemple $\\ex$\n",
    "STR_WEIGHT = r\"w\"\n",
    "STR_WEIGHT_CUR = r\"w_{ij}\"  # Poids de la connexion entre le neurone $j$ et le neurone $i$\n",
    "STR_ACTIVTHRES = r\"\\theta\"  # *Seuil d'activation* du neurone $i$\n",
    "STR_ACTIVFUNC = r\"f\"        # *Fonction d'activation* du neurone $i$\n",
    "STR_ERRFUNC = r\"E\"          # *Fonction objectif* ou *fonction d'erreur*\n",
    "STR_LEARNRATE = r\"\\epsilon\" # *Pas d'apprentissage* ou *Taux d'apprentissage*\n",
    "STR_LEARNIT = r\"n\"          # Numéro d'itération (ou cycle ou époque) du processus d'apprentissage\n",
    "STR_SIGIN = r\"x\"            # Signal de sortie du neurone $i$ pour l'exemple $\\ex$\n",
    "STR_SIGOUT = r\"y\"           # Signal de sortie du neurone $i$ pour l'exemple $\\ex$\n",
    "STR_SIGOUT_CUR = r\"y_i\"\n",
    "STR_SIGOUT_PREV = r\"y_j\"\n",
    "STR_SIGOUT_DES = r\"d\"           # Sortie désirée (*étiquette*) du neurone $i$ pour l'exemple $\\ex$\n",
    "STR_SIGOUT_DES_CUR = r\"d_i\"\n",
    "STR_WEIGHTS = r\"W\"              # Matrice des poids du réseau (en réalité il y a une matrice de taille potentiellement différente par couche)\n",
    "STR_ERRSIG = r\"\\Delta\"          # *Signal d'erreur* du neurone $i$ pour l'exemple $\\ex$\n",
    "\n",
    "def tex(tex_str):\n",
    "    return r\"$\" + tex_str + r\"$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = nnfig.init_figure(size_x=8, size_y=4)\n",
    "\n",
    "nnfig.draw_synapse(ax, (0, -6), (10, 0))\n",
    "nnfig.draw_synapse(ax, (0, -2), (10, 0))\n",
    "nnfig.draw_synapse(ax, (0, 2),  (10, 0))\n",
    "nnfig.draw_synapse(ax, (0, 6),  (10, 0), label=tex(STR_WEIGHT_CUR), label_position=0.5, fontsize=14)\n",
    "\n",
    "nnfig.draw_synapse(ax, (10, 0), (12, 0))\n",
    "\n",
    "nnfig.draw_neuron(ax, (0, -6), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0, -2), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0, 2),  0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0, 6),  0.5, empty=True)\n",
    "plt.text(x=0,    y=7.5,  s=tex(STR_PREV),        fontsize=14)\n",
    "plt.text(x=10,   y=1.5,  s=tex(STR_CUR),         fontsize=14)\n",
    "plt.text(x=0,    y=0,    s=r\"$\\vdots$\",          fontsize=14)\n",
    "plt.text(x=-2.5, y=0,    s=tex(STR_SIGOUT_PREV), fontsize=14)\n",
    "plt.text(x=13,   y=0,    s=tex(STR_SIGOUT_CUR),  fontsize=14)\n",
    "plt.text(x=9.2,  y=-1.8, s=tex(STR_POT_CUR),     fontsize=14)\n",
    "\n",
    "nnfig.draw_neuron(ax, (10, 0), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y = f\\left( \\sum_i \\weight_i \\feature_i \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\pot_\\cur = \\sum_\\prev \\wcur \\sigout_{\\prev}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigout_{\\cur} = \\activfunc(\\pot_\\cur)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\weights = \\begin{pmatrix}\n",
    "    \\weight_{11} & \\cdots & \\weight_{1m} \\\\\n",
    "    \\vdots       & \\ddots & \\vdots       \\\\\n",
    "    \\weight_{n1} & \\cdots & \\weight_{nm}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec :\n",
    "- $\\cur$: couche courante\n",
    "- $\\prev$: couche immédiatement en amont de la courche courrante (i.e. vers la couche d'entrée du réseau)\n",
    "- $\\next$: couche immédiatement en aval de la courche courrante (i.e. vers la couche de sortie du réseau)\n",
    "- $\\ex$: exemple (*sample* ou *feature*) courant (i.e. le vecteur des entrées courantes du réseau)\n",
    "- $\\pot_\\cur$: *Potentiel d'activation* du neurone $i$ pour l'exemple courant\n",
    "- $\\wcur$: Poids de la connexion entre le neurone $j$ et le neurone $i$\n",
    "- $\\activthres_\\cur$: *Seuil d'activation* du neurone $i$\n",
    "- $\\activfunc_\\cur$: *Fonction d'activation* du neurone $i$\n",
    "- $\\errfunc$: *Fonction objectif* ou *fonction d'erreur*\n",
    "- $\\learnrate$: *Pas d'apprentissage* ou *Taux d'apprentissage*\n",
    "- $\\learnit$: Numéro d'itération (ou cycle ou époque) du processus d'apprentissage\n",
    "- $\\sigout_\\cur$: Signal de sortie du neurone $i$ pour l'exemple courant\n",
    "- $\\sigoutdes_\\cur$: Sortie désirée (*étiquette*) du neurone $i$ pour l'exemple courant\n",
    "- $\\weights$: Matrice des poids du réseau (en réalité il y a une matrice de taille potentiellement différente par couche)\n",
    "- $\\errsig_i$: *Signal d'erreur* du neurone $i$ pour l'exemple courant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Fonction d'activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction sigmoïde\n",
    "\n",
    "La fonction sigmoïde (en forme de \"S\") est définie par :\n",
    "\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "pour tout réel $x$.\n",
    "\n",
    "On peut la généraliser à toute fonction dont l'expression est :\n",
    "\n",
    "$$f(x) = \\frac{1}{1 + e^{-\\lambda x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, _lambda=1.):\n",
    "    y = 1. / (1. + np.exp(-_lambda * x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(-5, 5, 300)\n",
    "\n",
    "y1 = sigmoid(x, 1.)\n",
    "y2 = sigmoid(x, 5.)\n",
    "y3 = sigmoid(x, 0.5)\n",
    "\n",
    "plt.plot(x, y1, label=r\"$\\lambda=1$\")\n",
    "plt.plot(x, y2, label=r\"$\\lambda=5$\")\n",
    "plt.plot(x, y3, label=r\"$\\lambda=0.5$\")\n",
    "\n",
    "plt.hlines(y=0, xmin=-5, xmax=5, color='gray', linestyles='dotted')\n",
    "plt.vlines(x=0, ymin=-2, ymax=2, color='gray', linestyles='dotted')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.title(\"Fonction sigmoïde\")\n",
    "plt.axis([-5, 5, -0.5, 2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tangente hyperbolique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    y = np.tanh(x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 300)\n",
    "y = tanh(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.hlines(y=0, xmin=-5, xmax=5, color='gray', linestyles='dotted')\n",
    "plt.vlines(x=0, ymin=-2, ymax=2, color='gray', linestyles='dotted')\n",
    "\n",
    "plt.title(\"Fonction tangente hyperbolique\")\n",
    "plt.axis([-5, 5, -2, 2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctions ayant pour expression\n",
    "\n",
    "$$\n",
    "f(t) = K \\frac{1}{1+ae^{-\\lambda t}}\n",
    "$$\n",
    "\n",
    "où $K$ et $\\lambda$ sont des réels positifs et $a$ un réel quelconque.\n",
    "\n",
    "Les fonctions sigmoïdes sont un cas particulier de fonctions logistique avec $a > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistique(x, a=1., k=1., _lambda=1.):\n",
    "    y = k / (1. + a * np.exp(-_lambda * x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(-5, 5, 300)\n",
    "\n",
    "y1 = logistique(x, a=1.)\n",
    "y2 = logistique(x, a=2.)\n",
    "y3 = logistique(x, a=0.5)\n",
    "\n",
    "plt.plot(x, y1, label=r\"$a=1$\")\n",
    "plt.plot(x, y2, label=r\"$a=2$\")\n",
    "plt.plot(x, y3, label=r\"$a=0.5$\")\n",
    "\n",
    "plt.hlines(y=0, xmin=-5, xmax=5, color='gray', linestyles='dotted')\n",
    "plt.vlines(x=0, ymin=-2, ymax=2, color='gray', linestyles='dotted')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.title(\"Fonction logistique\")\n",
    "plt.axis([-5, 5, -0.5, 2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le terme de biais\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = nnfig.init_figure(size_x=8, size_y=6)\n",
    "\n",
    "HSPACE = 6\n",
    "VSPACE = 4\n",
    "\n",
    "# Synapse #####################################\n",
    "\n",
    "#nnfig.draw_synapse(ax, (0,2*VSPACE), (HSPACE, 0), label=tex(STR_WEIGHT + \"_0\"), label_position=0.3)\n",
    "nnfig.draw_synapse(ax, (0,  VSPACE), (HSPACE, 0), label=tex(STR_WEIGHT + \"_1\"), label_position=0.3)\n",
    "nnfig.draw_synapse(ax, (0,       0), (HSPACE, 0), label=tex(STR_WEIGHT + \"_2\"), label_position=0.3)\n",
    "nnfig.draw_synapse(ax, (0, -VSPACE), (HSPACE, 0), label=tex(STR_WEIGHT + \"_3\"), label_position=0.3, label_offset_y=-0.8)\n",
    "\n",
    "nnfig.draw_synapse(ax, (HSPACE, 0), (HSPACE + 2, 0))\n",
    "\n",
    "# Neuron ######################################\n",
    "\n",
    "# Layer 1 (input)\n",
    "#nnfig.draw_neuron(ax, (0,2*VSPACE), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0,  VSPACE), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0,       0), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0, -VSPACE), 0.5, empty=True)\n",
    "\n",
    "# Layer 2\n",
    "nnfig.draw_neuron(ax, (HSPACE, 0), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "\n",
    "# Text ########################################\n",
    "\n",
    "# Layer 1 (input)\n",
    "#plt.text(x=0.5, y=VSPACE+1, s=tex(STR_SIGOUT + \"_i\"), fontsize=12)\n",
    "\n",
    "#plt.text(x=-1.7, y=2*VSPACE,    s=tex(\"1\"), fontsize=12)\n",
    "plt.text(x=-1.7, y=VSPACE,      s=tex(STR_SIGIN + \"_1\"), fontsize=12)\n",
    "plt.text(x=-1.7, y=-0.2,        s=tex(STR_SIGIN + \"_2\"), fontsize=12)\n",
    "plt.text(x=-1.7, y=-VSPACE-0.2, s=tex(STR_SIGIN + \"_3\"), fontsize=12)\n",
    "\n",
    "# Layer 2\n",
    "#plt.text(x=HSPACE-1.25, y=1.5, s=tex(STR_POT), fontsize=12)\n",
    "#plt.text(x=2*HSPACE+0.4,  y=1.5, s=tex(STR_SIGOUT + \"_o\"), fontsize=12)\n",
    "\n",
    "plt.text(x=HSPACE+2.5,  y=-0.3,\n",
    "         s=tex(STR_SIGOUT),\n",
    "         fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = nnfig.init_figure(size_x=8, size_y=6)\n",
    "\n",
    "HSPACE = 6\n",
    "VSPACE = 4\n",
    "\n",
    "# Synapse #####################################\n",
    "\n",
    "nnfig.draw_synapse(ax, (0,2*VSPACE), (HSPACE, 0), label=tex(STR_WEIGHT + \"_0\"), label_position=0.3)\n",
    "nnfig.draw_synapse(ax, (0,  VSPACE), (HSPACE, 0), label=tex(STR_WEIGHT + \"_1\"), label_position=0.3)\n",
    "nnfig.draw_synapse(ax, (0,       0), (HSPACE, 0), label=tex(STR_WEIGHT + \"_2\"), label_position=0.3)\n",
    "nnfig.draw_synapse(ax, (0, -VSPACE), (HSPACE, 0), label=tex(STR_WEIGHT + \"_3\"), label_position=0.3, label_offset_y=-0.8)\n",
    "\n",
    "nnfig.draw_synapse(ax, (HSPACE, 0), (HSPACE + 2, 0))\n",
    "\n",
    "# Neuron ######################################\n",
    "\n",
    "# Layer 1 (input)\n",
    "nnfig.draw_neuron(ax, (0,2*VSPACE), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0,  VSPACE), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0,       0), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0, -VSPACE), 0.5, empty=True)\n",
    "\n",
    "# Layer 2\n",
    "nnfig.draw_neuron(ax, (HSPACE, 0), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "\n",
    "# Text ########################################\n",
    "\n",
    "# Layer 1 (input)\n",
    "#plt.text(x=0.5, y=VSPACE+1, s=tex(STR_SIGOUT + \"_i\"), fontsize=12)\n",
    "\n",
    "plt.text(x=-1.7, y=2*VSPACE,    s=tex(\"1\"), fontsize=12)\n",
    "plt.text(x=-1.7, y=VSPACE,      s=tex(STR_SIGIN + \"_1\"), fontsize=12)\n",
    "plt.text(x=-1.7, y=-0.2,        s=tex(STR_SIGIN + \"_2\"), fontsize=12)\n",
    "plt.text(x=-1.7, y=-VSPACE-0.2, s=tex(STR_SIGIN + \"_3\"), fontsize=12)\n",
    "\n",
    "# Layer 2\n",
    "#plt.text(x=HSPACE-1.25, y=1.5, s=tex(STR_POT), fontsize=12)\n",
    "#plt.text(x=2*HSPACE+0.4,  y=1.5, s=tex(STR_SIGOUT + \"_o\"), fontsize=12)\n",
    "\n",
    "plt.text(x=HSPACE+2.5,  y=-0.3,\n",
    "         s=tex(STR_SIGOUT),\n",
    "         fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = nnfig.init_figure(size_x=8, size_y=6)\n",
    "\n",
    "HSPACE = 6\n",
    "VSPACE = 4\n",
    "\n",
    "# Synapse #####################################\n",
    "\n",
    "nnfig.draw_synapse(ax, (0,2*VSPACE), (HSPACE, 0), label=tex(STR_WEIGHT + \"_0\"), label_position=0.3)\n",
    "nnfig.draw_synapse(ax, (0,  VSPACE), (HSPACE, 0), label=tex(STR_WEIGHT + \"_1\"), label_position=0.3)\n",
    "nnfig.draw_synapse(ax, (0,       0), (HSPACE, 0), label=tex(STR_WEIGHT + \"_2\"), label_position=0.3)\n",
    "nnfig.draw_synapse(ax, (0, -VSPACE), (HSPACE, 0), label=tex(STR_WEIGHT + \"_3\"), label_position=0.3, label_offset_y=-0.8)\n",
    "\n",
    "nnfig.draw_synapse(ax, (HSPACE, 0), (HSPACE + 2, 0))\n",
    "\n",
    "# Neuron ######################################\n",
    "\n",
    "# Layer 1 (input)\n",
    "nnfig.draw_neuron(ax, (0,2*VSPACE), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0,  VSPACE), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0,       0), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0, -VSPACE), 0.5, empty=True)\n",
    "\n",
    "# Layer 2\n",
    "nnfig.draw_neuron(ax, (HSPACE, 0), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "\n",
    "# Text ########################################\n",
    "\n",
    "# Layer 1 (input)\n",
    "#plt.text(x=0.5, y=VSPACE+1, s=tex(STR_SIGOUT + \"_i\"), fontsize=12)\n",
    "\n",
    "plt.text(x=-1.7, y=2*VSPACE,    s=tex(\"1\"), fontsize=12)\n",
    "plt.text(x=-1.7, y=VSPACE,      s=tex(STR_SIGIN + \"_1\"), fontsize=12)\n",
    "plt.text(x=-1.7, y=-0.2,        s=tex(STR_SIGIN + \"_2\"), fontsize=12)\n",
    "plt.text(x=-1.7, y=-VSPACE-0.2, s=tex(STR_SIGIN + \"_3\"), fontsize=12)\n",
    "\n",
    "# Layer 2\n",
    "#plt.text(x=HSPACE-1.25, y=1.5, s=tex(STR_POT), fontsize=12)\n",
    "#plt.text(x=2*HSPACE+0.4,  y=1.5, s=tex(STR_SIGOUT + \"_o\"), fontsize=12)\n",
    "\n",
    "plt.text(x=HSPACE+2.5,  y=-0.3,\n",
    "         s=tex(STR_SIGOUT),\n",
    "         fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour vecteur d'entrée = ... et un vecteur de poids arbitrairement fixé à ...\n",
    "et un neurone défini avec la fonction sigmoïde, \n",
    "on peut calculer la valeur de sortie du neurone :\n",
    "\n",
    "On a:\n",
    "\n",
    "$$\n",
    "\\sum_i \\weight_i \\feature_i = \\dots\n",
    "$$\n",
    "donc\n",
    "$$\n",
    "y = \\frac{1}{1 + e^{-\\dots}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition d'un réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disposition des neurones en couches et couches cachées\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple : réseau de neurones à 1 couche \"cachée\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = nnfig.init_figure(size_x=8, size_y=4)\n",
    "\n",
    "HSPACE = 6\n",
    "VSPACE = 4\n",
    "\n",
    "# Synapse #####################################\n",
    "\n",
    "# Layer 1-2\n",
    "nnfig.draw_synapse(ax, (0,  VSPACE), (HSPACE,  VSPACE), label=tex(STR_WEIGHT + \"_1\"), label_position=0.4)\n",
    "nnfig.draw_synapse(ax, (0, -VSPACE), (HSPACE,  VSPACE), label=tex(STR_WEIGHT + \"_3\"), label_position=0.25, label_offset_y=-0.8)\n",
    "\n",
    "nnfig.draw_synapse(ax, (0,  VSPACE), (HSPACE, -VSPACE), label=tex(STR_WEIGHT + \"_2\"), label_position=0.25)\n",
    "nnfig.draw_synapse(ax, (0, -VSPACE), (HSPACE, -VSPACE), label=tex(STR_WEIGHT + \"_4\"), label_position=0.4, label_offset_y=-0.8)\n",
    "\n",
    "# Layer 2-3\n",
    "nnfig.draw_synapse(ax, (HSPACE,  VSPACE), (2*HSPACE, 0), label=tex(STR_WEIGHT + \"_5\"), label_position=0.4)\n",
    "nnfig.draw_synapse(ax, (HSPACE, -VSPACE), (2*HSPACE, 0), label=tex(STR_WEIGHT + \"_6\"), label_position=0.4, label_offset_y=-0.8)\n",
    "\n",
    "nnfig.draw_synapse(ax, (2*HSPACE, 0), (2*HSPACE + 2, 0))\n",
    "\n",
    "# Neuron ######################################\n",
    "\n",
    "# Layer 1 (input)\n",
    "nnfig.draw_neuron(ax, (0,  VSPACE), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0, -VSPACE), 0.5, empty=True)\n",
    "\n",
    "# Layer 2\n",
    "nnfig.draw_neuron(ax, (HSPACE,  VSPACE), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "nnfig.draw_neuron(ax, (HSPACE, -VSPACE), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "\n",
    "# Layer 3\n",
    "nnfig.draw_neuron(ax, (2*HSPACE, 0), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "\n",
    "# Text ########################################\n",
    "\n",
    "# Layer 1 (input)\n",
    "#plt.text(x=0.5, y=VSPACE+1, s=tex(STR_SIGOUT + \"_i\"), fontsize=12)\n",
    "plt.text(x=-1.7, y=VSPACE,      s=tex(STR_SIGIN + \"_1\"), fontsize=12)\n",
    "plt.text(x=-1.7, y=-VSPACE-0.2, s=tex(STR_SIGIN + \"_2\"), fontsize=12)\n",
    "\n",
    "# Layer 2\n",
    "#plt.text(x=HSPACE-1.25, y=VSPACE+1.5, s=tex(STR_POT + \"_1\"), fontsize=12)\n",
    "plt.text(x=HSPACE+0.4,  y=VSPACE+1.5, s=tex(STR_SIGOUT + \"_1\"), fontsize=12)\n",
    "\n",
    "#plt.text(x=HSPACE-1.25, y=-VSPACE-1.8, s=tex(STR_POT + \"_2\"), fontsize=12)\n",
    "plt.text(x=HSPACE+0.4,  y=-VSPACE-1.8, s=tex(STR_SIGOUT + \"_2\"), fontsize=12)\n",
    "\n",
    "# Layer 3\n",
    "#plt.text(x=2*HSPACE-1.25, y=1.5, s=tex(STR_POT + \"_o\"), fontsize=12)\n",
    "#plt.text(x=2*HSPACE+0.4,  y=1.5, s=tex(STR_SIGOUT + \"_o\"), fontsize=12)\n",
    "\n",
    "plt.text(x=2*HSPACE+2.5,  y=-0.3,\n",
    "         s=tex(STR_SIGOUT),\n",
    "         fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple : réseau de neurones à 2 couches \"cachée\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = nnfig.init_figure(size_x=8, size_y=4)\n",
    "\n",
    "HSPACE = 6\n",
    "VSPACE = 4\n",
    "\n",
    "# Synapse #####################################\n",
    "\n",
    "# Layer 1-2\n",
    "nnfig.draw_synapse(ax, (0,  VSPACE), (HSPACE,  VSPACE), label=tex(STR_WEIGHT + \"_1\"), label_position=0.4)\n",
    "nnfig.draw_synapse(ax, (0, -VSPACE), (HSPACE,  VSPACE), label=tex(STR_WEIGHT + \"_3\"), label_position=0.25, label_offset_y=-0.8)\n",
    "\n",
    "nnfig.draw_synapse(ax, (0,  VSPACE), (HSPACE, -VSPACE), label=tex(STR_WEIGHT + \"_2\"), label_position=0.25)\n",
    "nnfig.draw_synapse(ax, (0, -VSPACE), (HSPACE, -VSPACE), label=tex(STR_WEIGHT + \"_4\"), label_position=0.4, label_offset_y=-0.8)\n",
    "\n",
    "# Layer 2-3\n",
    "nnfig.draw_synapse(ax, (HSPACE,  VSPACE), (2*HSPACE,  VSPACE), label=tex(STR_WEIGHT + \"_5\"), label_position=0.4)\n",
    "nnfig.draw_synapse(ax, (HSPACE, -VSPACE), (2*HSPACE,  VSPACE), label=tex(STR_WEIGHT + \"_7\"), label_position=0.25, label_offset_y=-0.8)\n",
    "\n",
    "nnfig.draw_synapse(ax, (HSPACE,  VSPACE), (2*HSPACE, -VSPACE), label=tex(STR_WEIGHT + \"_6\"), label_position=0.25)\n",
    "nnfig.draw_synapse(ax, (HSPACE, -VSPACE), (2*HSPACE, -VSPACE), label=tex(STR_WEIGHT + \"_8\"), label_position=0.4, label_offset_y=-0.8)\n",
    "\n",
    "# Layer 3-4\n",
    "nnfig.draw_synapse(ax, (2*HSPACE,  VSPACE), (3*HSPACE, 0), label=tex(STR_WEIGHT + \"_9\"), label_position=0.4)\n",
    "nnfig.draw_synapse(ax, (2*HSPACE, -VSPACE), (3*HSPACE, 0), label=tex(STR_WEIGHT + \"_{10}\"), label_position=0.4, label_offset_y=-0.8)\n",
    "\n",
    "nnfig.draw_synapse(ax, (3*HSPACE, 0), (3*HSPACE + 2, 0))\n",
    "\n",
    "# Neuron ######################################\n",
    "\n",
    "# Layer 1 (input)\n",
    "nnfig.draw_neuron(ax, (0,  VSPACE), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0, -VSPACE), 0.5, empty=True)\n",
    "\n",
    "# Layer 2\n",
    "nnfig.draw_neuron(ax, (HSPACE,  VSPACE), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "nnfig.draw_neuron(ax, (HSPACE, -VSPACE), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "\n",
    "# Layer 3\n",
    "nnfig.draw_neuron(ax, (2*HSPACE,  VSPACE), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "nnfig.draw_neuron(ax, (2*HSPACE, -VSPACE), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "\n",
    "# Layer 4\n",
    "nnfig.draw_neuron(ax, (3*HSPACE, 0), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "\n",
    "# Text ########################################\n",
    "\n",
    "# Layer 1 (input)\n",
    "#plt.text(x=0.5, y=VSPACE+1, s=tex(STR_SIGOUT + \"_i\"), fontsize=12)\n",
    "plt.text(x=-1.7, y=VSPACE,      s=tex(STR_SIGIN + \"_1\"), fontsize=12)\n",
    "plt.text(x=-1.7, y=-VSPACE-0.2, s=tex(STR_SIGIN + \"_2\"), fontsize=12)\n",
    "\n",
    "# Layer 2\n",
    "#plt.text(x=HSPACE-1.25, y=VSPACE+1.5, s=tex(STR_POT + \"_1\"), fontsize=12)\n",
    "plt.text(x=HSPACE+0.4,  y=VSPACE+1.5, s=tex(STR_SIGOUT + \"_1\"), fontsize=12)\n",
    "\n",
    "#plt.text(x=HSPACE-1.25, y=-VSPACE-1.8, s=tex(STR_POT + \"_2\"), fontsize=12)\n",
    "plt.text(x=HSPACE+0.4,  y=-VSPACE-1.8, s=tex(STR_SIGOUT + \"_2\"), fontsize=12)\n",
    "\n",
    "# Layer 3\n",
    "#plt.text(x=2*HSPACE-1.25, y=VSPACE+1.5, s=tex(STR_POT + \"_3\"), fontsize=12)\n",
    "plt.text(x=2*HSPACE+0.4,  y=VSPACE+1.5, s=tex(STR_SIGOUT + \"_3\"), fontsize=12)\n",
    "\n",
    "#plt.text(x=2*HSPACE-1.25, y=-VSPACE-1.8, s=tex(STR_POT + \"_4\"), fontsize=12)\n",
    "plt.text(x=2*HSPACE+0.4,  y=-VSPACE-1.8, s=tex(STR_SIGOUT + \"_4\"), fontsize=12)\n",
    "\n",
    "# Layer 4\n",
    "plt.text(x=3*HSPACE-1.25, y=1.5, s=tex(STR_POT + \"_o\"), fontsize=12)\n",
    "#plt.text(x=3*HSPACE+0.4,  y=1.5, s=tex(STR_SIGOUT + \"_o\"), fontsize=12)\n",
    "\n",
    "plt.text(x=3*HSPACE+2.5,  y=-0.3,\n",
    "         s=tex(STR_SIGOUT),\n",
    "         fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pouvoir expressif d'un réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction objectif (ou fonction d'erreur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction objectif: $\\errfunc \\left( \\weights \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typiquement, la fonction objectif (fonction d'erreur) est la somme du carré de l'erreur de chaque neurone de sortie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\errfunc = \\frac12 \\sum_{\\cur \\in \\Omega} \\left[ \\sigout_\\cur - \\sigoutdes_\\cur \\right]^2\n",
    "$$\n",
    "\n",
    "$\\Omega$: l'ensemble des neurones de sortie\n",
    "\n",
    "Le $\\frac12$, c'est juste pour simplifier les calculs de la dérivée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = nnfig.init_figure(size_x=8, size_y=4)\n",
    "\n",
    "nnfig.draw_synapse(ax, (0, -6), (10, 0))\n",
    "nnfig.draw_synapse(ax, (0, -2), (10, 0))\n",
    "nnfig.draw_synapse(ax, (0, 2),  (10, 0))\n",
    "nnfig.draw_synapse(ax, (0, 6),  (10, 0))\n",
    "\n",
    "nnfig.draw_synapse(ax, (0, -6), (10, -4))\n",
    "nnfig.draw_synapse(ax, (0, -2), (10, -4))\n",
    "nnfig.draw_synapse(ax, (0, 2),  (10, -4))\n",
    "nnfig.draw_synapse(ax, (0, 6),  (10, -4))\n",
    "\n",
    "nnfig.draw_synapse(ax, (0, -6), (10, 4))\n",
    "nnfig.draw_synapse(ax, (0, -2), (10, 4))\n",
    "nnfig.draw_synapse(ax, (0, 2),  (10, 4))\n",
    "nnfig.draw_synapse(ax, (0, 6),  (10, 4))\n",
    "\n",
    "nnfig.draw_synapse(ax, (10, -4), (12, -4))\n",
    "nnfig.draw_synapse(ax, (10, 0), (12, 0))\n",
    "nnfig.draw_synapse(ax, (10, 4), (12, 4))\n",
    "\n",
    "nnfig.draw_neuron(ax, (0, -6), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0, -2), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0, 2),  0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0, 6),  0.5, empty=True)\n",
    "\n",
    "nnfig.draw_neuron(ax, (10, -4), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "nnfig.draw_neuron(ax, (10, 0),  1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "nnfig.draw_neuron(ax, (10, 4),  1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "\n",
    "plt.text(x=0, y=7.5, s=tex(STR_PREV), fontsize=14)\n",
    "plt.text(x=10, y=7.5, s=tex(STR_CUR), fontsize=14)\n",
    "\n",
    "plt.text(x=0,   y=0,    s=r\"$\\vdots$\", fontsize=14)\n",
    "plt.text(x=9.7, y=-6.1, s=r\"$\\vdots$\", fontsize=14)\n",
    "plt.text(x=9.7, y=5.8,  s=r\"$\\vdots$\", fontsize=14)\n",
    "\n",
    "plt.text(x=12.5, y=4,  s=tex(STR_SIGOUT + \"_1\"), fontsize=14)\n",
    "plt.text(x=12.5, y=0,  s=tex(STR_SIGOUT + \"_2\"), fontsize=14)\n",
    "plt.text(x=12.5, y=-4, s=tex(STR_SIGOUT + \"_3\"), fontsize=14)\n",
    "\n",
    "plt.text(x=16, y=4,  s=tex(STR_ERRFUNC + \"_1 = \" + STR_SIGOUT + \"_1 - \" + STR_SIGOUT_DES + \"_1\"), fontsize=14)\n",
    "plt.text(x=16, y=0,  s=tex(STR_ERRFUNC + \"_2 = \" + STR_SIGOUT + \"_2 - \" + STR_SIGOUT_DES + \"_2\"), fontsize=14)\n",
    "plt.text(x=16, y=-4, s=tex(STR_ERRFUNC + \"_3 = \" + STR_SIGOUT + \"_3 - \" + STR_SIGOUT_DES + \"_3\"), fontsize=14)\n",
    "\n",
    "plt.text(x=16, y=-8, s=tex(STR_ERRFUNC + \" = 1/2 ( \" + STR_ERRFUNC + \"^2_1 + \" + STR_ERRFUNC + \"^2_2 + \" + STR_ERRFUNC + \"^2_3 + \\dots )\"), fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mise à jours des poids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\weights_{\\learnit + 1} = \\weights_{\\learnit} - \\underbrace{\\learnrate \\nabla_{\\weights} \\errfunc \\left( \\weights_{\\learnit} \\right)}\n",
    "$$\n",
    "\n",
    "$- \\learnrate \\nabla_{\\weights} \\errfunc \\left( \\weights_{\\learnit} \\right)$: descend dans la direction opposée au gradient (plus forte pente)\n",
    "\n",
    "avec $\\nabla_{\\weights} \\errfunc \\left( \\weights_{\\learnit} \\right)$: gradient de la fonction objectif au point $\\weights$\n",
    "\n",
    "$\\learnrate > 0$: pas (ou taux) d'apprentissage\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_{\\wcur} & = \\wcur_{\\learnit + 1} - \\wcur_{\\learnit} \\\\\n",
    "               & = - \\learnrate \\frac{\\partial \\errfunc}{\\partial \\wcur}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Leftrightarrow \\wcur_{\\learnit + 1} = \\wcur_{\\learnit} - \\learnrate \\frac{\\partial \\errfunc}{\\partial \\wcur}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque présentation de l'ensemble des exemples = un *cycle* (ou une *époque*) d'apprentissage\n",
    "\n",
    "Critère d'arrêt de l'apprentissage: quand la valeur de la fonction objectif se stabilise (ou que le problème est résolu avec la précision souhaitée)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dérivée des principales fonctions d'activation\n",
    "\n",
    "##### Fonction sigmoïde\n",
    "\n",
    "Fonction dérivée :\n",
    "\n",
    "$$\n",
    "f'(x) = \\frac{\\lambda e^{-\\lambda x}}{(1+e^{-\\lambda x})^{2}}\n",
    "$$\n",
    "\n",
    "qui peut aussi être défini par\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} y}{\\mathrm{d} x} = \\lambda y (1-y)\n",
    "$$\n",
    "\n",
    "où $y$ varie de 0 à 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_sigmoid(x, _lambda=1.):\n",
    "    e = np.exp(-_lambda * x)\n",
    "    y = _lambda * e / np.power(1 + e, 2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(-5, 5, 300)\n",
    "\n",
    "y1 = d_sigmoid(x, 1.)\n",
    "y2 = d_sigmoid(x, 5.)\n",
    "y3 = d_sigmoid(x, 0.5)\n",
    "\n",
    "plt.plot(x, y1, label=r\"$\\lambda=1$\")\n",
    "plt.plot(x, y2, label=r\"$\\lambda=5$\")\n",
    "plt.plot(x, y3, label=r\"$\\lambda=0.5$\")\n",
    "\n",
    "plt.hlines(y=0, xmin=-5, xmax=5, color='gray', linestyles='dotted')\n",
    "plt.vlines(x=0, ymin=-2, ymax=2, color='gray', linestyles='dotted')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.title(\"Fonction dérivée de la sigmoïde\")\n",
    "plt.axis([-5, 5, -0.5, 2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tangente hyperbolique\n",
    "\n",
    "Dérivée :\n",
    "\n",
    "$$\n",
    "\\tanh '= \\frac{1}{\\cosh^{2}} = 1-\\tanh^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_tanh(x):\n",
    "    y = 1. - np.power(np.tanh(x), 2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 300)\n",
    "y = d_tanh(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.hlines(y=0, xmin=-5, xmax=5, color='gray', linestyles='dotted')\n",
    "plt.vlines(x=0, ymin=-2, ymax=2, color='gray', linestyles='dotted')\n",
    "\n",
    "plt.title(\"Fonction dérivée de la tangente hyperbolique\")\n",
    "plt.axis([-5, 5, -2, 2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - \"généralement le minimum local suffit\" (preuve ???)\n",
    "# - \"dans le cas contraire, le plus simple est de recommencer plusieurs fois l'apprentissage avec des poids initiaux différents et de conserver la meilleure matrice $\\weights$ (celle qui minimise $\\errfunc$)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 4))\n",
    "\n",
    "x = np.arange(10, 30, 0.1)\n",
    "y = (x - 20)**2 + 2\n",
    "\n",
    "ax.set_xlabel(r\"Poids $\" + STR_WEIGHTS + \"$\", fontsize=14)\n",
    "ax.set_ylabel(r\"Fonction objectif $\" + STR_ERRFUNC + \"$\", fontsize=14)\n",
    "\n",
    "# See http://matplotlib.org/api/axes_api.html#matplotlib.axes.Axes.tick_params\n",
    "ax.tick_params(axis='both',       # changes apply to the x and y axis\n",
    "               which='both',      # both major and minor ticks are affected\n",
    "               bottom='on',       # ticks along the bottom edge are on\n",
    "               top='off',         # ticks along the top edge are off\n",
    "               left='on',         # ticks along the left edge are on\n",
    "               right='off',       # ticks along the right edge are off\n",
    "               labelbottom='off', # labels along the bottom edge are off\n",
    "               labelleft='off')   # labels along the lefleft are off\n",
    "\n",
    "ax.set_xlim(left=10, right=25)\n",
    "ax.set_ylim(bottom=0, top=5)\n",
    "\n",
    "ax.plot(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "*Apprentissage incrémentiel* (ou *partiel*) (ang. *incremental learning*):\n",
    "on ajuste les poids $\\weights$ après la présentation d'un seul exemple\n",
    "(\"ce n'est pas une véritable descente de gradient\").\n",
    "C'est mieux pour éviter les minimums locaux, surtout si les exemples sont\n",
    "mélangés au début de chaque itération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# *Apprentissage différé* (ang. *batch learning*):\n",
    "# TODO\n",
    "# Est-ce que la fonction objectif $\\errfunc$ est une fonction multivariée\n",
    "# ou est-ce une aggrégation des erreurs de chaque exemple ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# **TODO: règle du delta / règle du delta généralisée**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rétropropagation du gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Rétropropagation du gradient*:\n",
    "une méthode pour calculer efficacement le gradient de la fonction objectif $\\errfunc$.\n",
    "\n",
    "Intuition:\n",
    "La *rétropropagation du gradient* n'est qu'une méthode parmis d'autre pour résoudre le probème d'optimisation des poids $\\weight$. On pourrait très bien résoudre ce problème d'optimisation avec des algorithmes évolutionnistes par exemple.\n",
    "En fait, l'intérêt de la méthode de la *rétropropagation du gradient* (et ce qui explique sa notoriété) est qu'elle formule le problème d'optimisation des poids avec une écriture analytique particulièrement efficace qui élimine astucieusement un grand nombre de calculs redondants (un peu à la manière de ce qui se fait en programmation dynamique): quand on decide d'optimiser les poids via une *descente de gradient*, certains termes (les *signaux d'erreurs* $\\errsig$) apparaissent un grand nombre de fois dans l'écriture analytique complète du gradient. La méthode de la retropropagation du gradient fait en sorte que ces termes ne soient calculés qu'une seule fois.\n",
    "À noter qu'on aurrait très bien pu résoudre le problème avec une descente de gradient oú le gradient $\\frac{\\partial \\errfunc}{\\partial\\wcur_{\\learnit}}$ serait calculé via une approximation numérique (*méthode des différences finies* par exemple) mais ce serait beaucoup plus lent et beaucoup moins efficace...\n",
    "\n",
    "Principe:\n",
    "on modifie les poids à l'aide des *signaux d'erreur* $\\errsig$.\n",
    "\n",
    "$$\n",
    "\\wcur_{\\learnit + 1} = \\wcur_{\\learnit} \\underbrace{- \\learnrate \\frac{\\partial \\errfunc}{\\partial \\wcur_{\\learnit}}}_{\\delta_\\prevcur}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_\\prevcur & = - \\learnrate \\frac{\\partial \\errfunc}{\\partial \\wcur(\\learnit)} \\\\\n",
    "                & = - \\learnrate \\errsig_\\cur \\sigout\\prev\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Dans le cas de *l'apprentissage différé* (*batch*), on calcule pour chaque exemple l'erreur correspondante. Leur contribution individuelle aux modifications des poids sont additionnées\n",
    "- L'apprentissage suppervisé fonctionne mieux avec des neurones de sortie linéaires (fonction d'activation $\\activfunc$ = fonction identitée) \"car les signaux d'erreurs se transmettent mieux\".\n",
    "- Des données d'entrée binaires doivent être choisies dans $\\{-1,1\\}$ plutôt que $\\{0,1\\}$ car un signal nul ne contribu pas à l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#Voc:\n",
    "#- *erreur marginale*: **TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note intéressante de Jürgen Schmidhuber : http://people.idsia.ch/~juergen/who-invented-backpropagation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signaux d'erreur $\\errsig_\\cur$ pour les neurones de sortie $(\\cur \\in \\Omega)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\errsig_\\cur = \\activfunc'(\\pot_\\cur)[\\sigout_\\cur - \\sigoutdes_\\cur]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signaux d'erreur $\\errsig_\\cur$ pour les neurones cachés $(\\cur \\not\\in \\Omega)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\errsig_\\cur = \\activfunc'(\\pot_\\cur) \\sum_\\next \\weight_\\curnext \\errsig_\\next\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = nnfig.init_figure(size_x=8, size_y=4)\n",
    "\n",
    "nnfig.draw_synapse(ax, (0, -2), (10, 0))\n",
    "nnfig.draw_synapse(ax, (0, 2),  (10, 0), label=tex(STR_WEIGHT + \"_{\" + STR_NEXT + STR_CUR + \"}\"), label_position=0.5, fontsize=14)\n",
    "\n",
    "nnfig.draw_synapse(ax, (10, 0), (12, 0))\n",
    "\n",
    "nnfig.draw_neuron(ax, (0, -2), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0, 2),  0.5, empty=True)\n",
    "\n",
    "plt.text(x=0, y=3.5, s=tex(STR_CUR), fontsize=14)\n",
    "plt.text(x=10, y=3.5, s=tex(STR_NEXT), fontsize=14)\n",
    "plt.text(x=0, y=-0.2, s=r\"$\\vdots$\", fontsize=14)\n",
    "\n",
    "nnfig.draw_neuron(ax, (10, 0), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plus de détail : calcul de $\\errsig_\\cur$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'exemple suivant on ne s'intéresse qu'aux poids $\\weight_1$, $\\weight_2$, $\\weight_3$, $\\weight_4$ et $\\weight_5$ pour simplifier la demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = nnfig.init_figure(size_x=8, size_y=4)\n",
    "\n",
    "HSPACE = 6\n",
    "VSPACE = 4\n",
    "\n",
    "# Synapse #####################################\n",
    "\n",
    "# Layer 1-2\n",
    "nnfig.draw_synapse(ax, (0,  VSPACE), (HSPACE,  VSPACE), label=tex(STR_WEIGHT + \"_1\"), label_position=0.4)\n",
    "nnfig.draw_synapse(ax, (0, -VSPACE), (HSPACE,  VSPACE), color=\"lightgray\")\n",
    "\n",
    "nnfig.draw_synapse(ax, (0,  VSPACE), (HSPACE, -VSPACE), color=\"lightgray\")\n",
    "nnfig.draw_synapse(ax, (0, -VSPACE), (HSPACE, -VSPACE), color=\"lightgray\")\n",
    "\n",
    "# Layer 2-3\n",
    "nnfig.draw_synapse(ax, (HSPACE,  VSPACE), (2*HSPACE,  VSPACE), label=tex(STR_WEIGHT + \"_2\"), label_position=0.4)\n",
    "nnfig.draw_synapse(ax, (HSPACE, -VSPACE), (2*HSPACE,  VSPACE), color=\"lightgray\")\n",
    "\n",
    "nnfig.draw_synapse(ax, (HSPACE,  VSPACE), (2*HSPACE, -VSPACE), label=tex(STR_WEIGHT + \"_3\"), label_position=0.4)\n",
    "nnfig.draw_synapse(ax, (HSPACE, -VSPACE), (2*HSPACE, -VSPACE), color=\"lightgray\")\n",
    "\n",
    "# Layer 3-4\n",
    "nnfig.draw_synapse(ax, (2*HSPACE,  VSPACE), (3*HSPACE, 0), label=tex(STR_WEIGHT + \"_4\"), label_position=0.4)\n",
    "nnfig.draw_synapse(ax, (2*HSPACE, -VSPACE), (3*HSPACE, 0), label=tex(STR_WEIGHT + \"_5\"), label_position=0.4, label_offset_y=-0.8)\n",
    "\n",
    "# Neuron ######################################\n",
    "\n",
    "# Layer 1 (input)\n",
    "nnfig.draw_neuron(ax, (0,  VSPACE), 0.5, empty=True)\n",
    "nnfig.draw_neuron(ax, (0, -VSPACE), 0.5, empty=True, line_color=\"lightgray\")\n",
    "\n",
    "# Layer 2\n",
    "nnfig.draw_neuron(ax, (HSPACE,  VSPACE), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "nnfig.draw_neuron(ax, (HSPACE, -VSPACE), 1, ag_func=\"sum\", tr_func=\"sigmoid\", line_color=\"lightgray\")\n",
    "\n",
    "# Layer 3\n",
    "nnfig.draw_neuron(ax, (2*HSPACE,  VSPACE), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "nnfig.draw_neuron(ax, (2*HSPACE, -VSPACE), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "\n",
    "# Layer 4\n",
    "nnfig.draw_neuron(ax, (3*HSPACE, 0), 1, ag_func=\"sum\", tr_func=\"sigmoid\")\n",
    "\n",
    "# Text ########################################\n",
    "\n",
    "# Layer 1 (input)\n",
    "plt.text(x=0.5, y=VSPACE+1, s=tex(STR_SIGOUT + \"_i\"), fontsize=12)\n",
    "\n",
    "# Layer 2\n",
    "plt.text(x=HSPACE-1.25, y=VSPACE+1.5, s=tex(STR_POT + \"_1\"), fontsize=12)\n",
    "plt.text(x=HSPACE+0.4,  y=VSPACE+1.5, s=tex(STR_SIGOUT + \"_1\"), fontsize=12)\n",
    "\n",
    "# Layer 3\n",
    "plt.text(x=2*HSPACE-1.25, y=VSPACE+1.5, s=tex(STR_POT + \"_2\"), fontsize=12)\n",
    "plt.text(x=2*HSPACE+0.4,  y=VSPACE+1.5, s=tex(STR_SIGOUT + \"_2\"), fontsize=12)\n",
    "\n",
    "plt.text(x=2*HSPACE-1.25, y=-VSPACE-1.8, s=tex(STR_POT + \"_3\"), fontsize=12)\n",
    "plt.text(x=2*HSPACE+0.4,  y=-VSPACE-1.8, s=tex(STR_SIGOUT + \"_3\"), fontsize=12)\n",
    "\n",
    "# Layer 4\n",
    "plt.text(x=3*HSPACE-1.25, y=1.5, s=tex(STR_POT + \"_o\"), fontsize=12)\n",
    "plt.text(x=3*HSPACE+0.4,  y=1.5, s=tex(STR_SIGOUT + \"_o\"), fontsize=12)\n",
    "\n",
    "plt.text(x=3*HSPACE+2,  y=-0.3,\n",
    "         s=tex(STR_ERRFUNC + \" = (\" + STR_SIGOUT + \"_o - \" + STR_SIGOUT_DES + \"_o)^2/2\"),\n",
    "         fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention**: $\\weight_1$ influe $\\pot_2$ **et** $\\pot_3$ en plus de $\\pot_1$ et $\\pot_o$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcul de la dérivée partielle de l'erreur par rapport au poid synaptique $\\weight_4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rappel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\errfunc  &= \\frac12 \\left( \\sigout_o - \\sigoutdes_o \\right)^2 \\tag{1} \\\\\n",
    "\\sigout_o &= \\activfunc(\\pot_o)  \\tag{2} \\\\\n",
    "\\pot_o    &= \\sigout_2 \\weight_4 + \\sigout_3 \\weight_5  \\tag{3} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c'est à dire:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\errfunc = \\frac12 \\left( \\activfunc \\left( \\sigout_2 \\weight_4 + \\sigout_3 \\weight_5 \\right) - \\sigoutdes_o \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donc, en appliquant les règles de derivation de fonctions composées, on a:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\errfunc}{\\partial \\weight_4} =\n",
    "\\frac{\\partial \\pot_o}{\\partial \\weight_4}\n",
    "\\underbrace{\n",
    "\\frac{\\partial \\sigout_o}{\\partial \\pot_o}\n",
    "\\frac{\\partial \\errfunc}{\\partial \\sigout_o}\n",
    "}_{\\errsig_o}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappel: dérivation des fonctions composées (parfois appelé règle de dérivation en chaîne ou règle de la chaîne)\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} y}{\\mathrm{d} x} = \\frac{\\mathrm{d} y}{\\mathrm{d} u} \\cdot \\frac{\\mathrm{d} u}{\\mathrm {d} x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "de (1), (2) et (3) on déduit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\pot_o}{\\partial \\weight_4}   &= \\sigout_2 \\\\\n",
    "\\frac{\\partial \\sigout_o}{\\partial \\pot_o}   &= \\activfunc'(\\pot_o) \\\\\n",
    "\\frac{\\partial \\errfunc}{\\partial \\sigout_o} &= \\sigout_o - \\sigoutdes_o \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le signal d'erreur s'écrit donc:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\errsig_o &=\n",
    "\\frac{\\partial \\sigout_o}{\\partial \\pot_o}\n",
    "\\frac{\\partial \\errfunc}{\\partial \\sigout_o} \\\\\n",
    "&= \\activfunc'(\\pot_o) [\\sigout_o - \\sigoutdes_o]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcul de la dérivée partielle de l'erreur par rapport au poid synaptique $\\weight_5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\errfunc}{\\partial \\weight_5} =\n",
    "\\frac{\\partial \\pot_o}{\\partial \\weight_5}\n",
    "\\underbrace{\n",
    "\\frac{\\partial \\sigout_o}{\\partial \\pot_o}\n",
    "\\frac{\\partial \\errfunc}{\\partial \\sigout_o}\n",
    "}_{\\errsig_o}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avec:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\pot_o}{\\partial \\weight_5}   &= \\sigout_3 \\\\\n",
    "\\frac{\\partial \\sigout_o}{\\partial \\pot_o}   &= \\activfunc'(\\pot_o) \\\\\n",
    "\\frac{\\partial \\errfunc}{\\partial \\sigout_o} &= \\sigout_o - \\sigoutdes_o \\\\\n",
    "\\errsig_o &=\n",
    "\\frac{\\partial \\sigout_o}{\\partial \\pot_o}\n",
    "\\frac{\\partial \\errfunc}{\\partial \\sigout_o} \\\\\n",
    "&= \\activfunc'(\\pot_o) [\\sigout_o - \\sigoutdes_o]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcul de la dérivée partielle de l'erreur par rapport au poid synaptique $\\weight_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\errfunc}{\\partial \\weight_2} =\n",
    "\\frac{\\partial \\pot_2}{\\partial \\weight_2}\n",
    "%\n",
    "\\underbrace{\n",
    " \\frac{\\partial \\sigout_2}{\\partial \\pot_2}\n",
    " \\frac{\\partial \\pot_o}{\\partial \\sigout_2}\n",
    " \\underbrace{\n",
    "  \\frac{\\partial \\sigout_o}{\\partial \\pot_o}\n",
    "  \\frac{\\partial \\errfunc}{\\partial \\sigout_o}\n",
    " }_{\\errsig_o}\n",
    "}_{\\errsig_2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avec:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\pot_2}{\\partial \\weight_2} &= \\sigout_1 \\\\\n",
    "\\frac{\\partial \\sigout_2}{\\partial \\pot_2} &= \\activfunc'(\\pot_2) \\\\\n",
    "\\frac{\\partial \\pot_o}{\\partial \\sigout_2} &= \\weight_4 \\\\\n",
    "\\errsig_2 &=\n",
    "\\frac{\\partial \\sigout_2}{\\partial \\pot_2}\n",
    "\\frac{\\partial \\pot_o}{\\partial \\sigout_2}\n",
    "\\errsig_o \\\\\n",
    "&= \\activfunc'(\\pot_2) \\weight_4 \\errsig_o\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcul de la dérivée partielle de l'erreur par rapport au poid synaptique $\\weight_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\errfunc}{\\partial \\weight_3} =\n",
    "\\frac{\\partial \\pot_3}{\\partial \\weight_3}\n",
    "%\n",
    "\\underbrace{\n",
    " \\frac{\\partial \\sigout_3}{\\partial \\pot_3}\n",
    " \\frac{\\partial \\pot_o}{\\partial \\sigout_3}\n",
    " \\underbrace{\n",
    "  \\frac{\\partial \\sigout_o}{\\partial \\pot_o}\n",
    "  \\frac{\\partial \\errfunc}{\\partial \\sigout_o}\n",
    " }_{\\errsig_o}\n",
    "}_{\\errsig_3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avec:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\pot_3}{\\partial \\weight_3} &= \\sigout_1 \\\\\n",
    "\\frac{\\partial \\sigout_3}{\\partial \\pot_3} &= \\activfunc'(\\pot_3) \\\\\n",
    "\\frac{\\partial \\pot_o}{\\partial \\sigout_3} &= \\weight_5 \\\\\n",
    "\\errsig_3 &= \n",
    "\\frac{\\partial \\sigout_3}{\\partial \\pot_3}\n",
    "\\frac{\\partial \\pot_o}{\\partial \\sigout_3}\n",
    "\\errsig_o \\\\\n",
    "&= \\activfunc'(\\pot_3) \\weight_5 \\errsig_o\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcul de la dérivée partielle de l'erreur par rapport au poid synaptique $\\weight_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\errfunc}{\\partial \\weight_1} =\n",
    "\\frac{\\partial \\pot_1}{\\partial \\weight_1}\n",
    "%\n",
    "\\underbrace{\n",
    " \\frac{\\partial \\sigout_1}{\\partial \\pot_1}\n",
    " \\left(\n",
    "  \\frac{\\partial \\pot_2}{\\partial \\sigout_1} % err?\n",
    "  \\underbrace{\n",
    "   \\frac{\\partial \\sigout_2}{\\partial \\pot_2}\n",
    "   \\frac{\\partial \\pot_o}{\\partial \\sigout_2}\n",
    "   \\underbrace{\n",
    "    \\frac{\\partial \\sigout_o}{\\partial \\pot_o}\n",
    "    \\frac{\\partial \\errfunc}{\\partial \\sigout_o}\n",
    "   }_{\\errsig_o}\n",
    "  }_{\\errsig_2}\n",
    "  +\n",
    "  \\frac{\\partial \\pot_3}{\\partial \\sigout_1} % err?\n",
    "  \\underbrace{\n",
    "   \\frac{\\partial \\sigout_3}{\\partial \\pot_3}\n",
    "   \\frac{\\partial \\pot_o}{\\partial \\sigout_3}\n",
    "   \\underbrace{\n",
    "    \\frac{\\partial \\sigout_o}{\\partial \\pot_o}\n",
    "    \\frac{\\partial \\errfunc}{\\partial \\sigout_o}\n",
    "   }_{\\errsig_o}\n",
    "  }_{\\errsig_3}\n",
    " \\right)\n",
    "}_{\\errsig_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avec:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\pot_1}{\\partial \\weight_1} &= \\sigout_i \\\\\n",
    "\\frac{\\partial \\sigout_1}{\\partial \\pot_1} &= \\activfunc'(\\pot_1) \\\\\n",
    "\\frac{\\partial \\pot_2}{\\partial \\sigout_1} &= \\weight_2 \\\\\n",
    "\\frac{\\partial \\pot_3}{\\partial \\sigout_1} &= \\weight_3 \\\\\n",
    "\\errsig_1 &=\n",
    "\\frac{\\partial \\sigout_1}{\\partial \\pot_1}\n",
    "\\left(\n",
    "\\frac{\\partial \\pot_2}{\\partial \\sigout_1}\n",
    "\\errsig_2\n",
    "+\n",
    "\\frac{\\partial \\pot_3}{\\partial \\sigout_1}\n",
    "\\errsig_3\n",
    "\\right) \\\\\n",
    "&= \n",
    "\\activfunc'(\\pot_1) \\left( \\weight_2 \\errsig_2 + \\weight_3 \\errsig_3 \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the activation function and its derivative\n",
    "activation_function = tanh\n",
    "d_activation_function = d_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(num_input_cells, num_output_cells, num_cell_per_hidden_layer, num_hidden_layers=1):\n",
    "    \"\"\"\n",
    "    The returned `weights` object is a list of weight matrices,\n",
    "    where weight matrix at index $i$ represents the weights between\n",
    "    layer $i$ and layer $i+1$.\n",
    "    \n",
    "    Numpy array shapes for e.g. num_input_cells=2, num_output_cells=2,\n",
    "    num_cell_per_hidden_layer=3 (without taking account bias):\n",
    "    - in:        (2,)\n",
    "    - in+bias:   (3,)\n",
    "    - w[0]:      (3,3)\n",
    "    - w[0]+bias: (3,4)\n",
    "    - w[1]:      (3,2)\n",
    "    - w[1]+bias: (4,2)\n",
    "    - out:       (2,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO:\n",
    "    # - faut-il que wij soit positif ?\n",
    "    # - loi normale plus appropriée que loi uniforme ?\n",
    "    # - quel sigma conseillé ?\n",
    "    \n",
    "    W = []\n",
    "    \n",
    "    # Weights between the input layer and the first hidden layer\n",
    "    W.append(np.random.uniform(low=0., high=1., size=(num_input_cells + 1, num_cell_per_hidden_layer + 1)))\n",
    "    \n",
    "    # Weights between hidden layers (if there are more than one hidden layer)\n",
    "    for layer in range(num_hidden_layers - 1):\n",
    "        W.append(np.random.uniform(low=0., high=1., size=(num_cell_per_hidden_layer + 1, num_cell_per_hidden_layer + 1)))\n",
    "    \n",
    "    # Weights between the last hidden layer and the output layer\n",
    "    W.append(np.random.uniform(low=0., high=1., size=(num_cell_per_hidden_layer + 1, num_output_cells)))\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_network(weights, input_signal):      # TODO: find a better name\n",
    "    \n",
    "    # Add the bias on the input layer\n",
    "    input_signal = np.concatenate([input_signal, [-1]])\n",
    "    \n",
    "    assert input_signal.ndim == 1\n",
    "    assert input_signal.shape[0] == weights[0].shape[0]\n",
    "    \n",
    "    # Compute the output of the first hidden layer\n",
    "    p = np.dot(input_signal, weights[0])\n",
    "    output_hidden_layer = activation_function(p)\n",
    "    \n",
    "    # Compute the output of the intermediate hidden layers\n",
    "    # TODO: check this\n",
    "    num_layers = len(weights)\n",
    "    for n in range(num_layers - 2):\n",
    "        p = np.dot(output_hidden_layer, weights[n + 1])\n",
    "        output_hidden_layer = activation_function(p)\n",
    "    \n",
    "    # Compute the output of the output layer\n",
    "    p = np.dot(output_hidden_layer, weights[-1])\n",
    "    output_signal = activation_function(p)\n",
    "    \n",
    "    return output_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient():\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_weights(num_input_cells=2, num_output_cells=2, num_cell_per_hidden_layer=3, num_hidden_layers=1)\n",
    "print(weights)\n",
    "#print(weights[0].shape)\n",
    "#print(weights[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_signal = np.array([.1, .2])\n",
    "input_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_network(weights, input_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le PMC peut approximer n'importe quelle fonction **continue** avec une précision arbitraire suivant le nombre de neurones présents sur la couche cachée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation des poids: généralement des petites valeurs aléatoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: la différence entre:\n",
    "# * réseau bouclé\n",
    "# * réseau récurent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes de la documentation sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *features* : les données d'entrée du réseau (i.e. les entrées de la 1ere couche du réseau)\n",
    "  - \"nombre de features\" =  taille du vecteur d'entrées\n",
    "- *loss function*: fonction objectif (ou fonction d'erreur)\n",
    "- *fitting*: processus d'apprentissage (training)\n",
    "- *sample*: exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les biais sont stockés dans une liste de vecteurs plutôt qu'une liste de scalaires... pourquoi ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avantages des PMC:\n",
    "- capables d'apprendre des modèles non linéaires\n",
    "- capables d'apprendre des modèles en temps réel (apprentissage *on-line*)\n",
    "\n",
    "Inconvenients des PMC:\n",
    "- les PMC avec une ou plusieurs couches cachées ont une fonction objectif non-convexe avec des minimas locaux. Par conséquent, le résultat du processus d'apprentissage peut varier d'une execution à l'autre suivant la valeur des poids initiaux et l'obtention d'un réseau optimal n'est pas garanti\n",
    "- pour obtenir un résultat satisfaisant, il est souvant nécessaire de régler (plus ou moins empiriquement) de nombreux meta-paramètres (nombres de couches cachées, nombre de neurones sur les couches cachées, nombres d'itérations, ...)\n",
    "- une  mauvaise normalisation des données d'entrée a un impact très négatif sur la qualité du résultat (\"mal conditionné\" ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cross-Entropy Loss Function*: ...\n",
    "\n",
    "*Softmax*: ...\n",
    "\n",
    "*Multi-label classification*: ... modèle de classifieur qui permet a un exemple d'appartenir à plusieurs classes"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
