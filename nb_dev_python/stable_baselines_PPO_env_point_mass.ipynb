{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My installtion instructions: https://gitlab.com/-/snippets/2057703"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/monitor_training.ipynb\n",
    "\n",
    "See also: https://stable-baselines.readthedocs.io/en/master/guide/examples.html#try-it-online-with-colab-notebooks\n",
    "\n",
    "# Stable Baselines, a Fork of OpenAI Baselines - Monitor Training and Plotting\n",
    "\n",
    "Github Repo: [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)\n",
    "\n",
    "Medium article: [https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82](https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82)\n",
    "\n",
    "[RL Baselines Zoo](https://github.com/araffin/rl-baselines-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines.\n",
    "\n",
    "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
    "\n",
    "Documentation is available online: [https://stable-baselines.readthedocs.io/](https://stable-baselines.readthedocs.io/)\n",
    "\n",
    "## Install Dependencies and Stable Baselines Using Pip\n",
    "\n",
    "List of full dependencies can be found in the [README](https://github.com/hill-a/stable-baselines).\n",
    "\n",
    "```\n",
    "sudo apt-get update && sudo apt-get install cmake libopenmpi-dev zlib1g-dev\n",
    "```\n",
    "\n",
    "```\n",
    "pip install stable-baselines[mpi]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C.f. https://stackoverflow.com/a/61318224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Stable Baselines only supports tensorflow 1.x for now\n",
    "%tensorflow_version 1.x\n",
    "!apt install swig cmake libopenmpi-dev zlib1g-dev xvfb x11-utils\n",
    "!pip install gym[box2d] pyvirtualdisplay PyOpenGL PyOpenGL-accelerate\n",
    "!pip install stable-baselines[mpi]==2.10.0 box2d box2d-kengz"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pyvirtualdisplay\n",
    "\n",
    "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "                                    size=(1400, 900))\n",
    "_ = _display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines\n",
    "stable_baselines.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Policy, RL agent, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gym\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines import results_plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Callback Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(\"Saving new best model to {}\".format(self.save_path))\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and wrap the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html\n",
    "# https://towardsdatascience.com/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e\n",
    "\n",
    "POSITION_MIN = -1000.\n",
    "POSITION_MAX =  1000.\n",
    "VELOCITY_MIN = -100.\n",
    "VELOCITY_MAX =  100.\n",
    "ACTION_MIN = -2.\n",
    "ACTION_MAX =  2.\n",
    "DT = 0.1\n",
    "MASS = 0.1\n",
    "MAX_STEPS = 1000\n",
    "\n",
    "class PointMassEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, mass=MASS, dt=DT):\n",
    "        super(PointMassEnv, self).__init__()    # Define action and observation space\n",
    "\n",
    "        self.mass = mass\n",
    "        self.dt = dt\n",
    "\n",
    "        self.position = None\n",
    "        self.velocity = None\n",
    "        self.current_step = None\n",
    "\n",
    "        self.viewer = None\n",
    "\n",
    "        # Actions: force\n",
    "        self.action_space = spaces.Box(low=ACTION_MIN, high=ACTION_MAX, shape=(1,), dtype=np.float32)\n",
    "\n",
    "        # State: (position, velocity)\n",
    "        self.observation_space = spaces.Box(low=np.array([VELOCITY_MIN, POSITION_MIN]), high=np.array([VELOCITY_MAX, POSITION_MAX]), dtype=np.float32)\n",
    "\n",
    "\n",
    "    def _reward(self, position):\n",
    "        # https://en.wikipedia.org/wiki/Gaussian_function\n",
    "        a = 1.      # The height of the curve's peak\n",
    "        b = 0.      # The position of the center of the peak\n",
    "        c = 100.     # The width of the \"bell\"\n",
    "        x = position\n",
    "        return a * np.exp(-(x - b)**2/(2.*c**2))\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute one time step within the environment\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Kinetics of point mass\n",
    "        force = action[0]\n",
    "        acceleration = force / self.mass\n",
    "        self.velocity += acceleration * self.dt\n",
    "        self.position += self.velocity * self.dt\n",
    "        #print(\"force: {}, acceleration: {}, velocity: {}, position: {}\".format(force, acceleration, self.velocity, self.position))\n",
    "\n",
    "        obs = np.array([self.velocity, self.position])\n",
    "\n",
    "        # Compute reward and done\n",
    "        reward = self._reward(self.position)\n",
    "        done = self.current_step > MAX_STEPS\n",
    "        \n",
    "        return obs, reward, done, {}\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        self.position = random.uniform(POSITION_MIN, POSITION_MAX)\n",
    "        self.velocity = 0.\n",
    "        self.current_step = 0\n",
    "\n",
    "        return np.array([self.velocity, self.position])\n",
    " \n",
    "\n",
    "#    def render(self, mode='human', close=False):\n",
    "#        # Render the environment to the screen\n",
    "#        print(self.velocity, self.position)\n",
    "# \n",
    "#\n",
    "#    def close(self):\n",
    "#        pass\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = POSITION_MAX - POSITION_MIN\n",
    "        scale = screen_width/world_width\n",
    "        carty = 100  # TOP OF CART\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "            axleoffset = cartheight / 4.0\n",
    "\n",
    "            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "\n",
    "            self.track = rendering.Line((0, carty), (screen_width, carty))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "        if self.position is None:\n",
    "            return None\n",
    "\n",
    "        x = self.position\n",
    "        cartx = x * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PointMassEnv()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = PointMassEnv()\n",
    "#env.reset()\n",
    "#a = env.action_space.sample()\n",
    "#s = env.step(a)\n",
    "#a, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##env = gym.make('CartPole-v1')\n",
    "#env = gym.make('MountainCarContinuous-v0')\n",
    "#env.reset()\n",
    "#a = env.action_space.sample()\n",
    "#s = env.step(a)\n",
    "#a, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "T = range(1000)\n",
    "\n",
    "for t in T:\n",
    "  action = np.array([2.])\n",
    "  next_velocity, next_pos = env.step(action)[0]\n",
    "  y.append(next_pos)\n",
    "\n",
    "plt.plot(list(T), y);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.arange(-100, 100, 0.1)\n",
    "#y = np.array([env._reward(_x) for _x in x])\n",
    "#plt.plot(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "#env = gym.make('CartPole-v1')\n",
    "env = PointMassEnv()\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "env = DummyVecEnv([lambda: env])  # PPO2 requires a vectorized environment to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train the PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the callback: check every 1000 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=100000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting helpers\n",
    "\n",
    "Stable Baselines has some built-in plotting helper, that you can find in `stable_baselines.results_plotter`. However, to show how to do it yourself, we are going to use custom plotting functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper from the library\n",
    "\n",
    "results_plotter.plot_results([log_dir], 1e5, results_plotter.X_TIMESTEPS, \"PPO Point Mass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, 'valid')\n",
    "\n",
    "\n",
    "def plot_results(log_folder, title='Learning Curve'):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    \"\"\"\n",
    "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
    "    #y = moving_average(y, window=50)\n",
    "    ## Truncate x\n",
    "    #x = x[len(x) - len(y):]\n",
    "\n",
    "    fig = plt.figure(title, figsize=(16,6))\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title(title + \" Smoothed\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a GIF of a Trained Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C.f. https://stable-baselines.readthedocs.io/en/master/guide/examples.html#bonus-make-a-gif-of-a-trained-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "obs = model.env.reset()\n",
    "img = model.env.render(mode='rgb_array')\n",
    "for i in range(350):\n",
    "    images.append(img)\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, _ ,_ = model.env.step(action)\n",
    "    img = model.env.render(mode='rgb_array')\n",
    "\n",
    "imageio.mimsave('ppo_point_mass_env.gif', [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(open('ppo_point_mass_env.gif','rb').read())    # https://stackoverflow.com/questions/61110188/how-to-display-a-gif-in-jupyter-notebook-using-google-colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._max_episode_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = []\n",
    "\n",
    "NUM_EPISODES = 100\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "\n",
    "    reward_sum = 0\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    " \n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        #env.render()           # Cannot render on Google Colab\n",
    "    \n",
    "    reward_list.append(reward_sum)\n",
    "\n",
    "print(\"Mean reward:\", sum(reward_list) / NUM_EPISODES)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
